<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>高斯分布的最大似然估计</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="emacsun" />
<link rel="stylesheet" type="text/css" href="../../css/worg.css" />
<a id="home" href="../../index.html"><img src="../../img/assets/home.png" ></a>
<a id="pdf"  href="./PRMLch2dot3-maximum-likelihood-for-gaussian.pdf"><img src="../../img/assets/pdf.png"></a>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "left",
        displayIndent: "3em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">高斯分布的最大似然估计</h1>
<div id="table-of-contents">
<h2>目录</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgf607f3b">1. 原理</a></li>
<li><a href="#orgca7ac93">2. 应用</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgf607f3b" class="outline-2">
<h2 id="orgf607f3b"><span class="section-number-2">1</span> 原理</h2>
<div class="outline-text-2" id="text-1">
<p>

</p>

<p>
给定一个数据集\(\mathbf{X} = (\mathbf{x}_{1},\ldots ,\mathbf{x}_{N})^{T}\)。假设\(\{\mathbf{x}_{n}\}\)是多变量高斯分布的一个独立的观察。我们可以通过最大似然估计来估计高斯分布的参数。对数似然函数是：
</p>
\begin{equation}
\label{eq:1}
\ln p(\mathbf{X}| \mathbf{\mu}, \mathbf{\Sigma}) = -\frac{ND}{2}\ln(2\pi) - \frac{N}{2}\ln|\mathbf{\Sigma}| - \frac{1}{2}\sum_{n=1}^{N}(\mathbf{x}_{n}- \mathbf{\mu})^{T}\mathbf{\Sigma}^{-1}(\mathbf{x}_{n} - \mathbf{\mu})
\end{equation}
<p>
对上式化简，我们发现似然函数对数据集合的依赖体现在\(\sum_{n=1}^{N}\mathbf{x}_{n}\)和\(\sum_{n=1}^{N}\mathbf{x}_{n}\mathbf{x}_{n}^{T}\)两个量上。这两个量叫做高斯分布的充分统计量（sufficient statistics）。不同的分布有不同的充分统计量，这个我们用到的时候在详谈，此处不展开。
</p>

<p>
在式 (\ref{eq:1})中，对\(\mathbf{\mu}\)求导，有：
</p>
\begin{equation}
\label{eq:2}
\frac{\partial }{\partial \mathbf{\mu}}\ln  p(\mathbf{X}| \mathbf{\mu}, \mathbf{\Sigma}) = \sum_{n=1}^{N} \sum_{n=1}^{N} \mathbf{\Sigma}^{-1} ( \mathbf{x}_{n} - \mathbf{\mu} )
\end{equation}
<p>
令上式为零，则我们得到了关于高斯分布均值的最大似然解：
</p>
\begin{equation}
\label{eq:3}
\mathbf{\mu}_{ML} = \frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_{n}
\end{equation}
<p>
显然，这个最大似然解是观测数据集合的均值。
</p>

<p>
对 (\ref{eq:1})的\(\mathbf{\Sigma}\)求导，有：
</p>
\begin{equation}
\label{eq:4}
\mathbf{\Sigma}_{ML} = \frac{1}{N} \sum_{n=1}^{N} (\mathbf{x}_{n}- \mathbf{\mu}_{ML})(\mathbf{x}_{n} - \mathbf{\mu}_{ML})^{T}
\end{equation}
<p>
式~(\ref{eq:4})中出现了\(\mathbf{\mu}_{ML}\)，这是联合优化\(\mathbf{\mu}\)和\(\mathbf{\Sigma}\)的结果。另外注意到\(\mathbf{\mu}_{ML}\)与\(\mathbf{\Sigma}_{ML}\)无关，所以我们可以先得到\(\mathbf{\mu}_{ML}\)，然后求\(\mathbf{\Sigma}_{ML}\)。
</p>

<p>
基于\(\mathbf{\mu}_{ML}\)和\(\mathbf{\Sigma}_{ML}\)，我们求高斯分布的期望和方差：
</p>
\begin{eqnarray}
\label{eq:5}
\mathbb{E}[\mathbf{\mu}_{ML}]&=& \mathbf{\mu} \\
\mathbb{E}[\mathbf{\Sigma}_{ML}]&=& \frac{N-1}{N} \mathbf{\Sigma}
\end{eqnarray}
<p>
我们发现最大似然估计的均值等于真实的均值，最大似然估计的方差总是小于真实值，因此这个估计是有偏的（biased）.我们可以定义一个不同的估计：
</p>
\begin{equation}
\label{eq:6}
\tilde{\mathbf{\Sigma}} = \frac{1}{N-1} \sum_{n=1}^{N} (\mathbf{x}_{n}- \mathbf{\mu}_{ML})(\mathbf{x}_{n} - \mathbf{\mu}_{ML})^{T}
\end{equation}
<p>
显然\(\tilde{\mathbf{\Sigma}}\)的期望与\(\mathbf{\Sigma}\)相等。
</p>
</div>
</div>
<div id="outline-container-orgca7ac93" class="outline-2">
<h2 id="orgca7ac93"><span class="section-number-2">2</span> 应用</h2>
<div class="outline-text-2" id="text-2">
<p>

</p>

<p>
以上讨论高斯分布参数的最大似然估计，这个过程为我们进行序贯估计（sequential estimation）提供了方便。序贯算法允许数据在线处理。所谓在线处理（on-line process）是指一次处理一个数据点然后丢点这个数据点。在线处理的优势是相对于离线处理（off-line）在线处理可以不用一次性保存并处理大量的数据。
</p>

<p>
考虑式 (\ref{eq:3})，对高斯分布均值的最大似然估计，如果我们把式(\ref{eq:3})写成递推的形式，则有：
</p>
\begin{eqnarray}
\label{eq:7}
\mathbf{\mu}_{ML}^{(N)}&=& \frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_{n} \\
&=& \frac{1}{N}\mathbf{x}_{N} + \frac{1}{N}\sum_{n=1}^{N-1}\mathbf{x}_{n}\\
&=& \frac{1}{N}\mathbf{x}_{N} + \frac{N-1}{N} \mathbf{\mu}_{ML}^{(N-1)} \\
&=& \mathbf{\mu}_{ML}^{(N-1)} + \frac{1}{N}(\mathbf{x}_{N} - \mathbf{\mu}_{ML}^{(N-1)})
\end{eqnarray}
<p>
这个结果提供了一个递推的求解高斯分布均值的方法。接收到第\(N-1\)个数据之后，我们对\(\mathbf{\mu}\)的估计\(\mathbf{\mu}_{ML}^{(N-1)}\)。我们现在观察到了\(\mathbf{x}_{N}\)，那么我们基于\(\mathbf{x}_{N}\)和\(\mathbf{\mu}_{ML}^{(N-1)}\)得到一个更新的\(\mathbf{\mu}_{ML}^{(N-1)}\)。仔细观察这个结果，我们发现相对于\(\mathbf{\mu}_{ML}^{(N-1)}\)，更新的\(\mathbf{\mu}_{ML}^{(N)}\)在原来的基础上更新了一个很小的量\(\frac{1}{N}(\mathbf{x}_{N} - \mathbf{\mu}_{ML}^{(N-1)})\)。
</p>

<p>
式 (\ref{eq:7})和式(\ref{eq:3})在本质上是相同的，提供了一种迭代计算均值的方法。但是在实际中我们却较少使用这种方法，我们更general的序贯学习方法。Robbins-Monro算法就是比较general的算法。考虑一对随机变量\(\theta\)和\(z\)，其联合概率分布是\(p(z,\theta)\).那么，给定\(\theta\)求\(z\)的条件期望确定了\(f(\theta)\)：
</p>
\begin{equation}
\label{eq:8}
f(\theta) = \mathbb{E}[z|\theta] = \int zp(z|\theta)\mathrm{d}z
\end{equation}
<p>
式 (\ref{eq:8}) 的结果可以用图<a href="#orgf081a1a">1</a>来表示。
</p>

<div id="orgf081a1a" class="figure">
<p><img src="../../img/computer_prml/20170616figure2dot10.png" alt="20170616figure2dot10.png" width="400" align="center" />
</p>
<p><span class="figure-number">图 1:</span> Robbins-Monro算法</p>
</div>

<p>
通过式~(\ref{eq:8})定义的函数叫做回归函数(regression functions). 定义了式 (\ref{eq:8})之后，我们的目标是找到\(\theta^{*}\)使得\(f(\theta^{*}) = 0\)。对于\(z\)和\(\theta\)，如果我们有一个较大的数据集。我们可以直接获得回归函数，并估计它的零点。
</p>

<p>
假设我们观测到了\(z\)的一个样本，然后我们期望得到对应的\(\theta^{*}\)的序贯估计。Robbins-Monro提供了一个过程。假设:
</p>
\begin{equation}
\label{eq:9}
\mathbb{E}[(z-f)^{2}|\theta] < \infty
\end{equation}
<p>
另外，不失一般性，我们认为\(f(\theta) > 0, \theta > \theta^{*}\), 且\( f(\theta ) < 0, \theta < \theta^{*} \), 就像图<a href="#orgf081a1a">1</a>所示的那样。Robbins=Monro过程定义了估计\(\theta^{*}\)的一个递推式：
</p>
\begin{equation}
\label{eq:10}
\theta^{(N)} = \theta^{(N-1)} + a_{N-1}z(\theta^{(N-1)})
\end{equation}
<p>
其中\(z(\theta^{(N)})\)是当\(\theta\)取值\(\theta^{(N)}\)时\(z\)的一个观测值。系数\(\{a_{N}\}\)代表一系列正数，满足：
</p>
\begin{eqnarray}
\label{eq:11}
\lim_{N\to \infty} a_{N}&=& 0 \\
\sum_{N=1}^{\infty}a_{N}&=& \infty \\
\sum_{N=1}^{\infty}a_{N}^{2}& < & \infty
\end{eqnarray}
<p>
Robbins和Monro证明了式 (\ref{eq:10})给出序贯估计的确可以以概率1收敛到\(\theta^{*}\)。
</p>

<p>
现在让我们仔细考虑使用Robbins-Monro算法如何可以让一个广义的最大似然估计问题收敛。我们知道，一句定义最大似然估计解\(\theta_{ML}\)是对数似然函数的一个静态点，满足：
</p>
\begin{equation}
\label{eq:12}
\frac{\partial}{\partial\theta } \bigg\{ \frac{1}{N}\sum_{n=1}^{N}\ln p(\mathbf{x}_{n}|\theta) \bigg\}{\huge{\lvert}}_{\theta_{ML}} = 0
\end{equation}
<p>
交换积分和求导顺序，令\(N\to \infty\),我们有：
</p>
\begin{equation}
\label{eq:13}
\lim_{N\to \infty}\frac{1}{N}\sum_{n=1}^{N}\frac{\partial}{\partial \theta}\ln p(x_{n}|\theta) = \mathbb{E}[\frac{\partial}{\partial \theta} \ln p(x|\theta)]
\end{equation}
<p>
因此我们看到找到最大似然解相当于找到回归函数的根。
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'zclspace';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>
</body>
</html>
